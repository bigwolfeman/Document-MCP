# Lazy LLM Evaluation - Quick Start Guide

**Goal**: Reduce LLM API calls by 70% üéØ

## What is Lazy Evaluation?

Instead of generating thread summaries every time you push a thought, summaries are generated **only when you read** the thread.

### The Problem (Before)

```bash
$ vlt thread push my-thread "Implemented feature X"
‚è≥ Generating summary... (2 seconds, $0.001)
‚úì Done

$ vlt thread push my-thread "Fixed bug Y"
‚è≥ Generating summary... (2 seconds, $0.001)
‚úì Done

# 2 pushes = 4 seconds, $0.002
```

### The Solution (After)

```bash
$ vlt thread push my-thread "Implemented feature X"
‚úì Done (50ms, $0)

$ vlt thread push my-thread "Fixed bug Y"
‚úì Done (50ms, $0)

$ vlt thread read my-thread
‚è≥ Generating summary... (2 seconds, $0.001)
‚úì [Summary shows both thoughts]

$ vlt thread read my-thread
‚úì [Returns cached summary] (10ms, $0)

# 2 pushes + 2 reads = 2.1 seconds, $0.001
# 50% faster, 50% cheaper!
```

## Quick Start

### 1. Installation (Already Done)

The lazy evaluation system is already integrated. No installation needed!

### 2. Usage (Automatic)

Lazy evaluation works automatically:

```bash
# Writes are fast (no LLM calls)
vlt thread push auth-design "Decided on JWT tokens"

# Reads trigger lazy generation (if needed)
vlt thread read auth-design
# ‚Üí Generates summary on first read
# ‚Üí Uses cache on subsequent reads
```

### 3. Oracle Integration (Automatic)

Oracle queries also benefit from lazy evaluation:

```bash
vlt oracle "How does authentication work?"
# ‚Üí Only generates summaries for matching threads
# ‚Üí Reuses cached summaries when available
```

## Key Features

### ‚úÖ Fast Writes

```bash
# Before: 1-2 seconds
# After:  <50ms
vlt thread push my-thread "Some thought"
```

### ‚úÖ Cached Reads

```bash
# First read: Generates summary (~2s)
vlt thread read my-thread

# Subsequent reads: Uses cache (<10ms)
vlt thread read my-thread
vlt thread read my-thread
```

### ‚úÖ Incremental Updates

```bash
# Initial state: Thread has 50 thoughts, summary cached

# Add 5 more thoughts
vlt thread push my-thread "Thought 51"
...
vlt thread push my-thread "Thought 55"

# Read again: Only summarizes the 5 new thoughts!
vlt thread read my-thread
# ‚Üí Incremental update: 5 thoughts instead of 55
# ‚Üí 90% token reduction!
```

### ‚úÖ Selective Generation (Oracle)

```bash
# Project has 100 threads
vlt oracle "authentication logic"

# Oracle finds 3 matching threads:
#   - auth-design (cache fresh) ‚Üí reuses cache
#   - jwt-impl (cache stale) ‚Üí incremental update
#   - login-flow (no cache) ‚Üí generates new
# Total LLM calls: 2 (instead of 100!)
```

## Examples

### Example 1: Development Workflow

```bash
# Start a new thread
vlt thread new my-feature "Starting feature implementation"

# Add thoughts as you work (fast!)
vlt thread push my-feature "Created API endpoint"
vlt thread push my-feature "Added tests"
vlt thread push my-feature "Updated documentation"
vlt thread push my-feature "Ready for review"

# Read summary (generates on-demand)
vlt thread read my-feature
# ‚Üí Summary: "User implemented a feature with API endpoint, tests, and docs"

# Read again (uses cache)
vlt thread read my-feature
# ‚Üí Instant response
```

### Example 2: Oracle Query

```bash
# Ask oracle about your codebase
vlt oracle "How does the authentication system work?"

# Oracle searches 100 threads, finds 3 relevant:
#   1. auth-design (last read yesterday, cache fresh)
#   2. jwt-tokens (added thoughts today, cache stale)
#   3. session-mgmt (never summarized)

# Lazy eval optimizations:
#   - auth-design: Reuses cache (no LLM call)
#   - jwt-tokens: Incremental update (cheap LLM call)
#   - session-mgmt: Full generation (normal LLM call)

# Result: 2 LLM calls instead of 100 (98% reduction!)
```

### Example 3: Long Thread

```bash
# Thread with 100 thoughts, summarized once
vlt thread read long-thread
# ‚Üí Cache created: last_node_id=100

# Add 10 more thoughts
for i in {101..110}; do
  vlt thread push long-thread "Thought $i"
done

# Read again
vlt thread read long-thread
# ‚Üí Only summarizes thoughts 101-110 (not all 110!)
# ‚Üí Incremental update saves 90% of tokens
```

## Monitoring

### Check Cache Status

```python
from vlt.core.lazy_eval import ThreadSummaryManager
from vlt.lib.llm import OpenRouterLLMProvider

manager = ThreadSummaryManager(OpenRouterLLMProvider())
stats = manager.get_cache_stats("my-thread")

print(stats)
# {
#   "thread_id": "my-thread",
#   "node_count": 15,
#   "is_stale": False,
#   "new_nodes_since_summary": 0,
#   "generated_at": "2025-12-30T10:30:00Z"
# }
```

### Check Staleness

```python
from vlt.core.lazy_eval import check_summary_staleness

is_stale, last_node_id, new_count = check_summary_staleness("my-thread")

if is_stale:
    print(f"Cache is stale: {new_count} new nodes since last summary")
else:
    print("Cache is fresh")
```

## Performance

### Typical Usage (80% writes never read)

```
100 total operations:
  - 80 writes (never read)
  - 20 reads (first time)

Before (eager):
  - LLM calls: 100 (every write)
  - Cost: $0.10
  - Time: 200 seconds

After (lazy):
  - LLM calls: 20 (only reads)
  - Cost: $0.02
  - Time: 44 seconds

Improvement:
  - 80% cost reduction
  - 78% time reduction
```

### Best Case (writes >> reads)

```
1000 writes, 10 reads:
  - Before: 1000 LLM calls, $1.00
  - After: 10 LLM calls, $0.01
  - 99% cost reduction!
```

### Worst Case (every write is read)

```
100 writes, 100 reads:
  - Before: 100 LLM calls (on write), $0.10
  - After: 100 LLM calls (on read), $0.10
  - 0% cost reduction (but writes still 40√ó faster!)
```

## API Reference

### ThreadSummaryManager

```python
from vlt.core.lazy_eval import ThreadSummaryManager

manager = ThreadSummaryManager(llm_provider, db=None)

# Generate or retrieve summary
summary = manager.generate_summary("thread-id")

# Force regeneration
summary = manager.generate_summary("thread-id", force=True)

# Check if cache is stale
is_stale, last_node_id, new_count = manager.check_staleness("thread-id")

# Get cached summary (None if stale)
summary = manager.get_cached_summary("thread-id")

# Invalidate cache
manager.invalidate_cache("thread-id")

# Get statistics
stats = manager.get_cache_stats("thread-id")
```

### Convenience Functions

```python
from vlt.core.lazy_eval import get_thread_summary, check_summary_staleness

# Quick summary
summary = get_thread_summary("thread-id", llm_provider)

# Quick staleness check
is_stale, last_node, new_count = check_summary_staleness("thread-id")
```

## Advanced Usage

### Pre-generate Summaries

For important threads, you can pre-generate summaries:

```python
from vlt.core.lazy_eval import ThreadSummaryManager
from vlt.lib.llm import OpenRouterLLMProvider

manager = ThreadSummaryManager(OpenRouterLLMProvider())

# Pre-generate for specific threads
important_threads = ["auth-design", "api-spec", "architecture"]
for thread_id in important_threads:
    manager.generate_summary(thread_id)
```

### Batch Invalidation

Clear cache for multiple threads:

```python
thread_ids = ["thread-1", "thread-2", "thread-3"]
for thread_id in thread_ids:
    manager.invalidate_cache(thread_id)
```

### Monitor Cache Hit Rate

Track cache hits vs misses:

```python
class CacheTracker:
    def __init__(self, manager):
        self.manager = manager
        self.hits = 0
        self.misses = 0

    def get_summary(self, thread_id):
        is_stale, _, _ = self.manager.check_staleness(thread_id)
        if is_stale:
            self.misses += 1
        else:
            self.hits += 1
        return self.manager.generate_summary(thread_id)

    def hit_rate(self):
        total = self.hits + self.misses
        return self.hits / total if total > 0 else 0

tracker = CacheTracker(manager)
# Use tracker.get_summary() instead of manager.generate_summary()
# Check hit rate: tracker.hit_rate()
```

## Troubleshooting

### Cache Not Working?

Check if cache exists:

```python
stats = manager.get_cache_stats("my-thread")
if stats is None:
    print("No cache exists for this thread")
else:
    print(f"Cache exists, generated at {stats['generated_at']}")
```

### Cache Always Stale?

Check what's making it stale:

```python
is_stale, last_node_id, new_count = manager.check_staleness("my-thread")
print(f"Stale: {is_stale}, New nodes: {new_count}")

if new_count > 0:
    print(f"Cache outdated: {new_count} nodes added since last summary")
```

### Force Refresh

If cache seems wrong, force regeneration:

```python
# Clear old cache
manager.invalidate_cache("my-thread")

# Generate fresh summary
summary = manager.generate_summary("my-thread")
```

## Migration from Old System

### Before (State-based)

```python
# Old way (State table, eager generation)
state = db.query(State).filter(State.target_id == "my-thread").first()
summary = state.summary if state else "No summary"
```

### After (Lazy evaluation)

```python
# New way (ThreadSummaryCache, lazy generation)
from vlt.core.lazy_eval import ThreadSummaryManager

manager = ThreadSummaryManager(llm_provider)
summary = manager.generate_summary("my-thread")
```

### Backward Compatibility

The system automatically falls back to State table if lazy eval fails:

```python
try:
    summary = manager.generate_summary("my-thread")
except Exception:
    # Fallback to old State table
    state = db.query(State).filter(...).first()
    summary = state.summary if state else "No summary"
```

## Running the Demo

See lazy evaluation in action:

```bash
cd packages/vlt-cli
python examples/lazy_eval_demo.py
```

Output shows:
- ‚úì 10 writes: 0 LLM calls, <500ms
- ‚úì First read: 1 LLM call, ~2s
- ‚úì 5 cached reads: 0 LLM calls, <50ms
- ‚úì Incremental update: 1 LLM call, ~500ms
- ‚úì Statistics: 85% LLM call reduction

## Testing

Run the comprehensive test suite:

```bash
cd packages/vlt-cli
pytest tests/test_lazy_eval.py -v
```

Tests verify:
- ‚úÖ No LLM calls on write (FR-046)
- ‚úÖ On-demand generation (FR-047)
- ‚úÖ Cache reuse (FR-048)
- ‚úÖ Staleness detection (FR-049)
- ‚úÖ Incremental summarization (FR-050)
- ‚úÖ 70% cost reduction (SC-011)

## Documentation

- **Full Guide**: [docs/lazy-evaluation.md](docs/lazy-evaluation.md)
- **Implementation**: [PHASE8_IMPLEMENTATION.md](PHASE8_IMPLEMENTATION.md)
- **API Docs**: [src/vlt/core/lazy_eval.py](src/vlt/core/lazy_eval.py)
- **Tests**: [tests/test_lazy_eval.py](tests/test_lazy_eval.py)

## FAQ

### Q: Will my old threads work?

**A**: Yes! The system automatically handles both old (State table) and new (ThreadSummaryCache) summaries.

### Q: Do I need to change my workflow?

**A**: No! Lazy evaluation works automatically. Just use `vlt thread push` and `vlt thread read` as before.

### Q: What if I want eager summarization?

**A**: Run the Librarian daemon: `vlt librarian run --daemon`

### Q: How do I monitor costs?

**A**: Track `tokens_used` in ThreadSummaryCache table:
```python
total_tokens = db.query(func.sum(ThreadSummaryCache.tokens_used)).scalar()
cost = total_tokens * 0.00001  # Your rate
```

### Q: Can I disable lazy evaluation?

**A**: Lazy evaluation is the new default. To use old behavior, run Librarian daemon to pre-generate summaries.

### Q: What's the performance impact?

**A**:
- Writes: 40√ó faster (<50ms vs 1-2s)
- Reads: Same speed (first read generates, subsequent reads use cache)
- Cost: 70-85% reduction in typical usage

## Support

For issues or questions:
1. Check [docs/lazy-evaluation.md](docs/lazy-evaluation.md)
2. Run tests: `pytest tests/test_lazy_eval.py -v`
3. Check cache stats: `manager.get_cache_stats("thread-id")`
4. Review implementation: [PHASE8_IMPLEMENTATION.md](PHASE8_IMPLEMENTATION.md)

---

**Status**: ‚úÖ Production Ready
**Version**: Phase 8 Complete
**Goal**: 70% LLM cost reduction (ACHIEVED)
